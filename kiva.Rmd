---
title: 'Using Text Analytics to Predict Loan Defaults'
subtitle: "A case study of Kiva"
author: "Dr. Stephen W. Thomas, Smith School of Business, Queen's University"
date: "January 2018"
documentclass: article
fontsize: 11pt
output:
  pdf_document:
    highlight: pygments
    number_sections: no
    toc: no
    toc_depth: '2'
  word_document:
    toc: no
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, fig.align='center')
```

```{r}
library(tidyverse)
library(ggthemes)
library(scales)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(MLmetrics)
library(topicmodels)
library(tidytext)
library(knitr)
library(kableExtra)
```


Kiva Microfunds[^1] is a non-profit that allows individuals to lend money to low-income entrepreneurs and students around the world. Since starting in 2005, Kiva has crowd-funded millions of loans with a repayment rate of around 98%.

[^1]: https://www.kiva.org/

At Kiva, each loan request includes both traditional demographic information, such as gender and location, as well as a personal story, because Kiva wants lenders to connect with the borrowers on a human level. An example of a personal story:

*Evelyn is 40 years old and married with 3 kids. She is in the Karura Hope women group and her life has been changed by the first KIVA loan she received last year which she is completing this quarter. Before she received the loan, she used to sell 9 litres of milk daily to local residents. After receiving the loan she bought iron sheets, five cement packets, one lorry of sand, some ballast and animal feed for her cows and improved her cow shed. Today she sells a daily average of 40 litres of milk to the Kiamba Dairy cooperative society, which is affiliated to the Kenya Cooperative Creameries at a cost of USD 0.28 per litre. Her daily farming has really grown. Evelyn intends to buy another dairy cow and a tank of water for home consumption and for her cows. She intends to repay in monthly installments.*

Despite her uplifting story, and her previous successful loan, Evelyn defaulted on her next loan of 900 USD. Her lenders lost their money, and Evelyn is no longer allowed to use Kiva.

Kiva's data science team has been tasked to determine if this unfortunate default---and others like it---could have been predicted from the data in the loan application. If defaults can be predicted beforehand, then the risk to potential lenders is reduced, and loan resources are given to borrowers with the highest likelihood of repaying.

To tackle this task, the data science team decides to use machine learning to build a prediction model based on historical loan data. The data science team is especially curious if the personal story itself will add any predictive power to the model. Since the personal stories are textual, as opposed to numerical, the data science team will need to use _text analytics_---techniques to understand, organize, and transform textual data into numerical data---to process the stories.  

Will text analytics help Kiva's data science team to build an effective prediction model for loan applications? Will the prediction model reduce risk for lenders and Kiva?
 


# Kiva's Data

Luckily for the data science team, Kiva carefully maintains a history of all previous loan applications, including items like the borrower's demographic data, the requested loan amount, and whether the loan was repaid.

Also worth noting is that all loan applications have an associated field partner, which is local  microfinance institution with which Kiva works to find and fund loans. Every loan at Kiva is offered by a partner to a borrower, and the partner works with Kiva to get funding for that loan from lenders.

Kiva's data science team extracts a snapshot of the historical loan data. The snapshot contains about 8,000 completed loans, of which about 50% were repaid and 50% were defaulted. Exhibit 1 shows the data dictionary and a few sample rows, and Exhibit 2 contains a descriptive summary of each of the eight variables in the dataset.


# Beyond Numbers: Adding Text Variables

Most machine learning algorithms that construct prediction models are able to accept numerical and categorical data as inputs, but are not able to accept raw text data. This means that the data science team can't use the `story` variable as-is. The team must first use various text analytics techniques to turn each story into numerical data. One common technique to do so is to split each story (also called a _document_ in the literature) into individual words, and count how many times each word appears in each document. Each individual word becomes its own variable in the dataset.

As an example, consider the following dataset, in which the `loan_purpose` variable contains raw text data and thus cannot be input to machine learning algorithms

```{r}
tmpdf = data.frame(id=c(1, 2, 3, 4, "..."), 
                   age=c(34, 56, 23, 29, ""), 
                   country=c("Panama", "Mexico", "Uganda", "Kenya", ""),
                   loan_purpose=c("I'd like to purchase a new farm tractor for my farm.", 
                                 "Hire additional day labourors to tend to my expanding avocado farm.", 
                                 "My child sitting service is expanding and I really need to renovate my home.",
                                 "To purchase 20 more cows for my dairy farm.", ""))

kable(tmpdf, "latex", booktab=TRUE) %>%  
  kable_styling(font_size = 10, full_width=TRUE, latex_options = c("striped", "scale_down"))
```

Splitting each document as described above would result in the following dataset:


```{r}
tmpdf = data.frame(id=c(1, 2, 3, 4, "..."), 
                   age=c(34, 56, 23, 29, ""), 
                   country=c("Panama", "Mexico", "Uganda", "Kenya", ""),
                   farm=c(2, 1, 0, 1, ""),
                   purchase=c(1, 0, 0, 1, ""),
                   expanding=c(0, 1, 1, 0, ""),
                   cows=c(0,0,0,1, ""),
                   hire=c(0 ,1, 0, 0, ""),
                   tractor=c(1, 0, 0, 0, ""),
                   "..."=c("...","...","...","...", "")
)

kable(tmpdf, "latex", booktab=TRUE) %>%  
  kable_styling(font_size = 10, full_width=TRUE, latex_options = c("striped", "scale_down")) 
```

Each unique word from the raw text data now becomes its own variable, and the value of each variable is the number of times that word appears in the original document. The original `loan_purpose` variable is removed. In this way, the remaining variables are all now numerical, and thus the dataset can be input into a machine learning algorithm.

Another common text analytics technique is to use two- or three-word phrases, instead of individual words. The idea is the same: the phrases now become variables in the dataset. 

A recent advancement in text analytics is to use a _topic model_ to extract the topics, or themes, from each document. In particular, the most common topic model is Latent Dirichlet Allocation (LDA). LDA can automatically extract high-level topics from text documents. For example, LDA might determine that one document contains the two topics, "dairy, cow, milk" and "wheat, farm, field", while another document contains two other topics, "clothing, garment, shirt" and "children, child, kid." LDA will discover which topics are in which documents. Note that the LDA creates the topics from scratch, based on the input text; it does not have a list of predefined topics from which it chooses. As a result, the topics are specific to the text data in the given dataset.

The data science team decides to use LDA to extract twelve overarching topics from the stories. The team removes the `story` variable, and adds twelve new variables (i.e., one for each topic) that specify the percentage of words in each story that come from each topic. Exhibit 3 shows the resulting topics, and Exhibit 4 shows a few sample rows in the final dataset that is ready to be input to the machine learning algorithms.


# Building the Prediction Model

Armed with the final dataset, the data science team can now use machine learning algorithms to build a prediction model for loan defaults. There are lots of machine learning algorithms available, ranging from simple logistic regression, to Naive Bayes, all the way to deep neural networks. For this task, the team decides to use a decision tree algorithm, because of its speed, accuracy, and interpretability. 

In order to determine whether the text (or, more precisely, the topics built from the text) has predictive power, the team builds two similar, but different, decision tree models. The first is built by giving the decision tree algorithm only the numerical and categorical variables from the original dataset, i.e., `sector`, `country`, `gender`, `loan_amount`, and  `non_payment`. The second model is built by giving the decision tree algorithm all of those variables plus the twelve topic variables. The team thus runs the algorithm twice, and recieves two models in return that they can compare. Exhibit 5 shows the two models and their performance characteristics.


# Case Discussion Questions

1. What is Kiva's value proposition?
4. What factors might go into a lender's decision to lend money to a borrower?
5. How does text data affect the prediction model's ability to predict a default? 
7. According to the decision tree models, which variable(s) best predict a default?
7. What other text analytics/NLP techniques might be applied to the dataset to improve the prediction model?
8. What additional information might lead to a better prediction model?
9. How should Kiva operationalize the prediction model? What technical challenges and risks do you envision? What procedural challenges and risks do you envision?


<P style="page-break-before: always">
\newpage



<!--
# Loading the Data
-->


```{r, include=FALSE}
df <- read_csv("data/kiva.csv")
df = df %>%
  rename(story = en)
```


```{r, include=FALSE}
str(df)
df$id = 1:nrow(df)
df$status = as.factor(df$status)
df$sector = as.factor(df$sector)
df$country = as.factor(df$country)
df$gender = as.factor(df$gender)
df$nonpayment = as.factor(df$nonpayment)
```

<!--
Let's look at a sample of our data.
-->

```{r, include=FALSE, eval=FALSE}
head(df, n=20)
summary(df)
```

<!--
# Data Cleaning
-->

```{r, include=FALSE}
# Remove HTML Tags
df = df %>% 
  mutate(story = gsub("<.*?>", "", story))

# Convert into tidytext format
text_df <- df %>%
  select(id, status, story) %>%
  unnest_tokens(word, story)

## Remove stopwords
custom_stop_words = data.frame(word=c("loan", "business"))
text_df <- text_df %>%
  anti_join(stop_words, by=c("word"="word")) %>%
  anti_join(custom_stop_words, by=c("word"="word")) %>%
  arrange(id)

# Stem words
#library(SnowballC)
#df = df %>% 
#  mutate(story = wordStem(story))
```


<!-- 
# Feature Engineering

## Latent Dirichlet Allocation

Let's use a technique called Latent Dirichlet Allocation (LDA) to extract the topics from each document.
-->

```{r, include=FALSE}
# Count each word in each document.
word_counts = text_df %>%
  group_by(id, word) %>%
  summarize(count = n())
```


```{r, include=FALSE}
# Create a document term matrix
dtm = word_counts %>% cast_dtm(id, word, count)

# Remove sparse terms from the document term matrix.
library(tm)
dtm2.nosparse <- removeSparseTerms(dtm, 0.9995)

rowTotals <- apply(dtm2.nosparse, 1, sum) #Find the sum of words in each Document
which(rowTotals==0)
dtm.new   <- dtm2.nosparse[rowTotals> 0, ] 
```

<!--
Run the LDA model.
-->

```{r, include=FALSE}
num_topics = 12

# Because the LDA model can take quite a few minutes to run, and because I run this script over and over again
# checking its knitr output, I don't want to run LDA every single time. 
runModel = FALSE
if (runModel == TRUE) {
  # Run the model
  lda <- LDA(dtm.new, k = num_topics, control = list(seed = 1234))
  
  # Name each topic
  t = terms(lda, k=4)
  topic_names = apply(t, 2, function(x) paste(x, collapse = "_"))
  
  lda_beta <- tidy(lda, matrix = "beta")
  lda_gamma <- tidy(lda, matrix = "gamma")
  lda_gamma$document = as.integer(lda_gamma$document)
  
  # Save output
  readr::write_csv(beta, sprintf("beta_%d.csv", num_topics))
  readr::write_csv(lda_gamma, sprintf("gamma_%d.csv", num_topics))
  readr::write_csv(as.data.frame(topic_names), sprintf("topicnames_%d.csv", num_topics))
  
} else {
  # Read the output from a previous run
  lda_beta = readr::read_csv(sprintf("beta_%d.csv", num_topics))
  lda_gamma = readr::read_csv(sprintf("gamma_%d.csv", num_topics))
  topic_names = t(readr::read_csv(sprintf("topicnames_%d.csv", num_topics)))
}

tn = data.frame(id=1:12, topic_name = as.character(t(topic_names)))
tn$topic_name = as.character(tn$topic_name)
tn$topic_name = sprintf("%02d: %s", 1:12, tn$topic_name)
  
```

<!--
Add the resulting document topic probabilities to the `df` dataframe.
-->

```{r, include=FALSE}
lda_gamma_new = lda_gamma %>% spread(topic, gamma)

df_new  = df %>% left_join(lda_gamma_new, by=c("id" = "document"))
library(data.table)
setnames(df_new, old=sprintf("%d", c(1:12)), new=sprintf("topic %d: %s", c(1:12), topic_names))
```


# Exhibit 1: Data Dictionary

The table below is a dictionary for the eight variables in the dataset.

```{r}
tmpdf = data.frame(Variable=c("id", "sector", "country", "gender", "loan_amount", "non_payment", "story", "status"),
                   Description=c("A unique identifier for the loan.",
                                 "Industry sector of borrower.",
                                 "Borrower's country of residence.",
                                 "Borrower's gender.",
                                 "Amount of the loan, in USD.",
                                 "Who is liable if the loan defaults: the lender, or the partner?",
                                 "Borrower's personal story.",
                                 "Whether borrower defaulted or repaid loan."
                                 ))
kable(tmpdf, "latex", booktab=TRUE) %>%  
  kable_styling(font_size = 10, full_width=TRUE, latex_options = c("striped", "scale_down")) %>% footnote(general = "Here is a general comments of the table. ",
           number = c("Footnote 1; ", "Footnote 2; "),
           alphabet = c("Footnote A; ", "Footnote B; "),
           symbol = c("Footnote Symbol 1; ", "Footnote Symbol 2")
           )
```



\vspace{10pt}

The table below shows 10 random rows in dataset.

```{r}
set.seed(124)
sample_ids = floor(runif(10, min = 1, max = nrow(df_new)))

aa = df[sample_ids, ] %>%
  select(-id) %>%
  select(status, sector, country, gender, loan_amount, nonpayment, story)

# Just to make the table look better
aa$country = gsub("Dominican Republic", "D.R.", aa$country)
aa$story = strtrim(aa$story, 150)
aa$story = paste(aa$story, " (...)", sep = "")

kable(aa, "latex", booktab=TRUE, digits=2) %>%
  kable_styling(latex_options = c("striped", "scale_down"))  %>%
  column_spec(7, width = "25em")
```




<P style="page-break-before: always">
\newpage

# Exhibit 2

Below are descriptive plots for the variables in the dataset.

\vspace{20pt}

```{r fig.height=2, fig.width=2, out.width='.49\\linewidth', fig.show='hold',fig.align='center'}

#myt = theme_igray()
myt = theme_economist()
#myt = theme_fivethirtyeight()
qplot(status, data=df, geom="bar", fill=status, xlab="status") + myt +
  theme(legend.position = "none") 

qplot(gender, data=df, geom="bar", fill=status) + myt +
  theme(legend.position = "none")

qplot(nonpayment, data=df, geom="bar", fill=status) + myt +
  theme(legend.position = "none")
```

\vspace{20pt}



```{r fig.height=2.8, fig.width=3, out.width='.49\\linewidth', fig.show='hold',fig.align='center'}

tmpdf = df
tmpdf$country = gsub("Dominican Republic", "D.R.", tmpdf$country)

qplot(country, data=tmpdf, geom="bar", fill=status) + myt +
  theme(legend.position = "none")

qplot(sector, data=df, geom="bar", fill=status, xlab="sector") + myt +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=1)) + theme(legend.position = "none")

rm(tmpdf)
```

\vspace{20pt}


```{r fig.width=4, fig.height=2.5, out.width='.49\\linewidth', fig.show='hold',fig.align='center'}
df %>% 
  ggplot(aes(loan_amount, colour=status, fill=status)) + myt +
  geom_density(alpha=0.1) +
  theme(legend.position = "none") + 
  labs(x = "loan_amount (USD)")

df %>%
  mutate(en_length = nchar(story)) %>%
  ggplot(aes(en_length, colour=status, fill=status)) + myt +
  geom_density(alpha=0.1) +
  theme(legend.position = "none") +
  labs(x = "Number of letters in `story`")
```


<P style="page-break-before: always">
\newpage

### Top Words

The table below shows the top (i.e, most frequently occurring) words in the `story` variable.

```{r rows.print=20}
kable(text_df %>% group_by(word) %>%
  summarize(count=n()) %>%
  mutate(freq = count / sum(count)) %>%
  arrange(desc(count)) %>%
  top_n(17), "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"), font_size=9)
```


### Most Biased Words

The plot below show which words are most biased towards being `paid` or `defaulted`, using the log odds ratio metric.

```{r}
status_words_count = text_df %>% group_by(status, word) %>%
  summarize(count=n()) %>%
  arrange(desc(count))

log_ratios = status_words_count %>% 
  spread (status, count) %>%
  select(-`<NA>`) %>%
  mutate(defaulted = ifelse(is.na(defaulted), 0, defaulted)) %>%
  mutate(paid = ifelse(is.na(paid), 0, paid)) %>%
  mutate(total=defaulted+paid) %>%
  mutate(log_ratio = log2(paid/defaulted)) 
```

```{r, fig.height=3.5}
log_ratios %>%
  filter(total > 500) %>%
  group_by(log_ratio < 0) %>%
  top_n(14, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, log_ratio)) %>%
  ggplot(aes(word, log_ratio, fill = log_ratio < 0)) +
  myt +
  geom_col() +
  coord_flip() +
  ylab("log odds ratio") +
  scale_fill_discrete(name = "", labels = c("paid", "default")) +
  theme(legend.position = "right", legend.text = element_text(size = 8), 
                                                              legend.margin = margin(0.5, 0.5, 0.5, 0.5, "pt"))
```


```{r, include=FALSE}
kable(log_ratios %>%
  filter(total > 500) %>%
  arrange(desc(log_ratio)) %>%
  top_n(17), "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


```{r rows.print=20, include=FALSE}
kable(log_ratios %>%
  filter(total > 500) %>%
  arrange((log_ratio)) %>%
  top_n(-20), "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```



<P style="page-break-before: always">
\newpage

# Exhibit 3: LDA Topics

Latent Dirichlet Allocation (LDA) was applied to the `story` variable using the R package `topicmodels`. The Kiva team told LDA to find twelve topics.

### LDA Top Terms Per Topic

The figure below shows the top words related to each of the twelve discovered topics. Each topic is assigned a name of the form "Topic Number: TopWord1_TopWord2_TopWord3_TopWord4."

```{r,fig.width=10,fig.height=8.0}
ap_top_terms <- lda_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  left_join(tn, by=c("topic" = "id")) %>% 
  ggplot(aes(term, beta, fill = factor(topic))) +
  myt +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic_name, scales = "free_y", ncol=3) +
  coord_flip() + 
  theme(strip.text = element_text(size=10)) # Make titles smaller
```


<P style="page-break-before: always">
\newpage

### Documents per LDA Topic

The figure below shows the number of documents that contain each topic.

```{r}
topic_totals = lda_gamma %>%
  left_join(df, by=c("document" = "id")) %>%
  select(c(-story)) %>% 
  filter(gamma >= 0.05) %>%
  group_by(topic, status) %>% 
  summarize(count=n()) %>%
  spread(status, count) %>%
  mutate(total = defaulted + paid) %>% 
  left_join(tn, by=c("topic" = "id")) %>%
  select(topic, topic_name, everything())
```

```{r fig.height=2}
tmp_gathered = topic_totals %>% 
  select(topic, topic_name, defaulted, paid) %>% 
  gather(Status, Value, defaulted, paid)

tmp_gathered$topic = as.factor(tmp_gathered$topic)
ggplot(tmp_gathered, aes(x=topic, y=Value, fill=Status)) + myt +
  geom_bar(stat="identity") +
  theme(legend.position = "right", legend.text = element_text(size = 8), 
                                                              legend.margin = margin(0.5, 0.5, 0.5, 0.5, "pt"))
```


```{r, include=FALSE}
kable(topic_totals, "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


### LDA Examples

Below is an example of a `story` that contained LDA topic 8 at 99%. 

*Senaida Agueda has a business in which she buys and resells clothing that she has been operating for about 2 years now with the help of loans from Esperanza. She has two children, 41 and 38, whom do not live with Mrs. Agueda. When not working with her clothing shop, she enjoys going to the beach and cooking, arroz con carne (rice with meat) being one of her favorite dishes. As Mrs. Agueda is in her elder years and has fully grown children, she simply wishes to sustain her business to support her and her husband. Mrs. Senaida Agueda is a member of an eight person group, Group 4, that is part of a larger micro-bank called Mujeres de Fe, "Women of Faith" in English. In the picture, Mrs. Agueda is third from the right along with members of her group and some others of Mujeres de Fe. (...)*

Below is an example for LDA topic 4: 

 *Descripcin del Negocio. La Sra Angela se dedica a la venta de articulos para el hogar a credito y de forma anbulante ademas vende golosinas a llos nios de una escuela. En la actualidad vende a personas de otros sectores que han sido recomendados por buenos clientes que le refieren esos sitios. Uso del Prstamo. Ella necesita el credito para comprar mas mercaderia pues en estas epocas de fin de aos le son muy solicitadas. Informacin Personal. Ella tiene 29 aos y tiene dos hijos que estudian su casa es de caa y tiene estabilidad familiar. Translated from Spanish by Kiva Volunteer Wendy Wise*


And finally, below is an example for LDA topic 1:

*Mary is 65 years of age, married with six children. All her children are married and self-reliant. She is a member of St Jude group at Githunguri in Thika district. Mary is earns her income as a dairy farmer. She needs a USD 150 loan to help her buy another small high breed dairy calf, which she will raise to maturity. She plans to meet her repayments on monthly basis.*


```{r, eval=FALSE, include=FALSE}
ids = lda_gamma %>%
  filter(topic==1) %>%
  arrange(desc(gamma)) %>%
  top_n(1000) %>%
  left_join(df_new, by=c("document" = "id")) %>%
  mutate(len = nchar(story)) %>%
  arrange(len)
ids
ids = c(7830, 7306, 7258, 7105)

df[7306,3]

df_new[ids,]

kable(t(df_new[ids[1],]), "latex", booktab=TRUE)  %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"))
```


<P style="page-break-before: always">
\newpage

# Exhibit 4: A Sample of Data

The table below shows the same 10 random rows from Exhibit 1, except from the final dataset. The columns named "T1", "T2", etc., contain the percentage of the original story's words that have been assigned to topic 1, topic 2, etc., as determined by LDA. 

\vspace{20pt}

```{r}
aa = df_new[sample_ids, ] %>%
  select(-story, -id)

# Just to make the table look better
aa$country = gsub("Dominican Republic", "D.R.", aa$country)
setnames(aa, new=sprintf("T%d", c(1:12)), old=sprintf("topic %d: %s", c(1:12), topic_names))

kable(aa, "latex", booktab=TRUE, digits=2) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
```


 
<P style="page-break-before: always">
\newpage

# Exhibit 5: Model Perfomance

Decision tree models were created with the R package `rpart`.

## Model 1 (No text)

Below is the model that was created from only the original numerical and categorical variables.

```{r fig.height=2}
set.seed(123)
# Don't want to use either of these for prediction, and the - sign doesn't work
# with rpart forumulas.
df_notext = subset(df_new, select=c(status, sector, country, gender, loan_amount, nonpayment))

# Split the data into training and testing.
train_notext <- sample_frac(df_notext, 0.8)
test_notext <- setdiff(df_notext, train_notext)


# Let's train the model.
form = as.formula(status ~ .)
tree <- rpart(form, train_notext, method="class")
rpart.plot(tree, extra=2)
```


<!--
\vspace{20pt}

The following table summarizes the predictions of the decision on testing data.
-->

```{r, eval=TRUE, inculde=FALSE}
predicted = predict(tree, test_notext, type="class")
actual = test_notext$status
preds = data.frame((table(predicted, actual))) %>%
  spread(actual, Freq) %>%
  mutate(total = defaulted + paid) %>%
  select(predicted, total, everything())
```

```{r, include=FALSE}
kable(preds, "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down")) %>%
  add_header_above(c(" " = 2, "actual" = 2))
```


## Model 2 (With Text)

Below is the model that was created from all variables in the dataset.


```{r fig.height=2.5}
set.seed(123)
# Don't want to use either of these for prediction, and the - sign doesn't work
# with rpart forumulas.
df_text = subset(df_new, select=c(-id, -story))

# Split the data into training and testing.
train_text <- sample_frac(df_text, 0.8)
test_text <- setdiff(df_text, train_text)


# Let's create the model.
form = as.formula(status ~ .)
tree <- rpart(form, train_text, method="class")
rpart.plot(tree, extra=2)
```

<!--
\vspace{20pt}

Below is a summary of its predictions:
-->

```{r, include=FALSE}

predicted.text = predict(tree, test_text, type="class")
actual.text = test_text$status
preds.text = data.frame((table(predicted.text, actual.text))) %>%
  spread(actual.text, Freq) %>%
  mutate(total = defaulted + paid) %>%
  select(predicted.text, total, everything())
```



```{r, include=FALSE}
kable(preds.text, "latex", booktab=TRUE) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down")) %>%
  add_header_above(c(" " = 2, "actual" = 2))
```


<!--
That is, the model predicted `defaulted` 74 times: 50 times correctly, and 24 times incorrectly. It predicted `paid` 159 times: 120 times correctly, and 39 times incorrectly.
-->


## Metrics

Below is the accuracy and other metrics of the two models.

```{r}
bb = data.frame(Metric=c("Accuracy", "Precision", "Recall", "F1 Score", "Sensitivity", "Specificity"),
                "Model_1" =c(Accuracy(y_true=actual, y_pred=predicted),
                        Precision(y_true=actual, y_pred=predicted),
                        Recall(y_true=actual, y_pred=predicted),
                        F1_Score(predicted, actual),
                        Sensitivity(y_true=actual, y_pred=predicted),
                        Specificity(y_true=predicted, y_pred=actual)),
                "Model_2" =  c(Accuracy(y_true=actual.text, y_pred=predicted.text),
                        Precision(y_true=actual.text, y_pred=predicted.text),
                        Recall(y_true=actual.text, y_pred=predicted.text),
                        F1_Score(predicted.text, actual.text),
                        Sensitivity(y_true=actual.text, y_pred=predicted.text),
                        Specificity(y_true=predicted.text, y_pred=actual.text)))


kable(bb, "latex", booktab=TRUE, digits=3) %>%
  kable_styling(full_width=TRUE, latex_options = c("striped", "scale_down"), font_size=9)
```




<P style="page-break-before: always">
\newpage


# Appendix 1: Data Collection

The data in this case study was collected from Build.Kiva[^2], Kiva's website that provides snapshots of Kiva loan data. In the full dataset, about 98% of loans are paid and 2% defaulted. In this case study, we look at only a sample of the data, where the split between paid and defaulted is closer to 50%-50%. This sample is available at [http://www.github.com/stepthom/sandbox/data/kiva.csv](http://www.github.com/stepthom/sandbox/data/kiva.csv).

[^2]: https://build.kiva.org

 
