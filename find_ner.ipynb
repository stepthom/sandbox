{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\st50\\AppData\\Local\\Continuum\\anaconda3\\envs\\small_sklearn\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.1). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_named_entities(dataframe1, ID_COL_NAME=\"id\", TEXT_COL_NAME = \"tweet_text\", DATE_COL_NAME=\"date\", include_date=True, include_text=True):   \n",
    "    output = []\n",
    "    \n",
    "    for index, row in dataframe1.iterrows():\n",
    "        if isinstance(row[TEXT_COL_NAME], str) and row[TEXT_COL_NAME]:\n",
    "            doc = nlp(row[TEXT_COL_NAME])\n",
    "            \n",
    "            for e in doc.ents:\n",
    "                cur_dict = {}\n",
    "                cur_dict[ID_COL_NAME] = row[ID_COL_NAME]\n",
    "                    \n",
    "                if include_date:\n",
    "                    cur_dict[DATE_COL_NAME] = row[DATE_COL_NAME]\n",
    "                    \n",
    "                if include_text:\n",
    "                    cur_dict[TEXT_COL_NAME] = row[TEXT_COL_NAME]\n",
    "\n",
    "                    \n",
    "                cur_dict['Entity'] = e.text\n",
    "                cur_dict['Label'] = e.label_\n",
    "                cur_dict['Start'] = e.start_char\n",
    "                cur_dict['End'] = e.end_char\n",
    "                                   \n",
    "                output.append(cur_dict)\n",
    "                \n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it(file_base, id_col=\"id\", text_col=\"tweet_text\", date_col=\"date\", include_date=True, include_text=True):\n",
    "    df = pd.read_csv('data/'+file_base+'.csv')\n",
    "    # Drop rows without any text\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    df2 = find_named_entities(df, id_col, text_col, date_col, include_date, include_text)\n",
    "    df2.to_csv('out/'+file_base+'_ner.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"obama_tweets\", id_col=\"id\", text_col=\"tweet_text\", date_col='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"elonmusk_tweets\", id_col=\"id\", text_col=\"text\", date_col='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"2017_trump_tweets\", id_col=\"id\", text_col=\"tweet\", date_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"JoeBidenTweets\", id_col=\"id\", text_col=\"tweet\", date_col='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"pence_tweets\", id_col=\"id\", text_col=\"tweet_text\", date_col='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"Hillary_Emails\", id_col=\"Id\", text_col=\"ExtractedBodyText\", date_col='ExtractedDateSent', include_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"reutersCSV\", id_col=\"pid\", text_col=\"doc.text\", date_col=None, include_date=False, include_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_it(file_base=\"kiva_cleaned\", id_col=\"loan_id\", text_col=\"en_clean\", date_col=None, include_date=False, include_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_sklearn_kernel",
   "language": "python",
   "name": "small_sklearn_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
